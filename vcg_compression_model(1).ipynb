{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fcUHGx2bExlZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import wfdb\n",
        "import pywt\n",
        "from scipy import signal\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGtoVCGConverter:\n",
        "    \"\"\"\n",
        "    Converts ECG signals to VCG using Frank's transformation\n",
        "    Frank's lead system provides orthogonal X, Y, Z components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Frank's transformation coefficients for standard 12-lead ECG\n",
        "        # These convert standard ECG leads to orthogonal VCG (X, Y, Z)\n",
        "        self.frank_coefficients = {\n",
        "            'X': {'I': 0.610, 'II': -0.171, 'V1': -0.781, 'V2': -0.516,\n",
        "                  'V3': -0.044, 'V4': 0.456, 'V5': 0.815, 'V6': 0.891},\n",
        "            'Y': {'I': -0.233, 'II': 0.887, 'V1': 0.022, 'V2': -0.106,\n",
        "                  'V3': -0.229, 'V4': -0.310, 'V5': -0.246, 'V6': -0.063},\n",
        "            'Z': {'I': 0.127, 'II': 0.022, 'V1': -0.229, 'V2': -0.310,\n",
        "                  'V3': -0.246, 'V4': -0.063, 'V5': 0.055, 'V6': 0.108}\n",
        "        }\n",
        "\n",
        "    def load_ptb_record(self, record_path):\n",
        "        \"\"\"\n",
        "        Load PTB Diagnostic ECG record (.dat format)\n",
        "\n",
        "        Args:\n",
        "            record_path: Path to PTB record (without extension)\n",
        "\n",
        "        Returns:\n",
        "            signals: ECG signals array\n",
        "            fields: Metadata dictionary\n",
        "        \"\"\"\n",
        "        try:\n",
        "            record = wfdb.rdrecord(record_path)\n",
        "            return record.p_signal, record.__dict__\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading record {record_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def convert_to_vcg(self, ecg_signals, lead_names):\n",
        "        \"\"\"\n",
        "        Convert multi-lead ECG to 3-lead VCG (X, Y, Z)\n",
        "\n",
        "        For PTB database which has 15 leads, we'll use a simplified approach:\n",
        "        - Use leads I, II, and V1-V6 if available\n",
        "        - If not all leads available, use available leads with adjusted coefficients\n",
        "\n",
        "        Args:\n",
        "            ecg_signals: Array of shape (samples, leads)\n",
        "            lead_names: List of lead names corresponding to columns\n",
        "\n",
        "        Returns:\n",
        "            vcg_signals: Array of shape (samples, 3) representing X, Y, Z\n",
        "        \"\"\"\n",
        "        n_samples = ecg_signals.shape[0]\n",
        "        vcg_signals = np.zeros((n_samples, 3))\n",
        "\n",
        "        # Create a mapping of lead names to signal columns\n",
        "        lead_map = {name.strip().upper(): idx for idx, name in enumerate(lead_names)}\n",
        "\n",
        "        # Simplified VCG derivation for PTB database\n",
        "        # PTB has leads: 'i', 'ii', 'iii', 'avr', 'avl', 'avf', 'v1'-'v6'\n",
        "\n",
        "        # X component (Left-Right): Primarily from lead I and chest leads\n",
        "        if 'I' in lead_map:\n",
        "            vcg_signals[:, 0] += 0.5 * ecg_signals[:, lead_map['I']]\n",
        "        if 'V1' in lead_map:\n",
        "            vcg_signals[:, 0] += -0.3 * ecg_signals[:, lead_map['V1']]\n",
        "        if 'V6' in lead_map:\n",
        "            vcg_signals[:, 0] += 0.4 * ecg_signals[:, lead_map['V6']]\n",
        "\n",
        "        # Y component (Superior-Inferior): Primarily from lead II and aVF\n",
        "        if 'II' in lead_map:\n",
        "            vcg_signals[:, 1] += 0.6 * ecg_signals[:, lead_map['II']]\n",
        "        if 'AVF' in lead_map:\n",
        "            vcg_signals[:, 1] += 0.4 * ecg_signals[:, lead_map['AVF']]\n",
        "\n",
        "        # Z component (Anterior-Posterior): Primarily from chest leads\n",
        "        if 'V1' in lead_map:\n",
        "            vcg_signals[:, 2] += 0.3 * ecg_signals[:, lead_map['V1']]\n",
        "        if 'V2' in lead_map:\n",
        "            vcg_signals[:, 2] += 0.2 * ecg_signals[:, lead_map['V2']]\n",
        "        if 'V5' in lead_map:\n",
        "            vcg_signals[:, 2] += -0.3 * ecg_signals[:, lead_map['V5']]\n",
        "        if 'V6' in lead_map:\n",
        "            vcg_signals[:, 2] += -0.2 * ecg_signals[:, lead_map['V6']]\n",
        "\n",
        "        return vcg_signals\n",
        "\n",
        "    def process_ptb_database(self, data_dir, output_dir, max_records=None):\n",
        "        \"\"\"\n",
        "        Process entire PTB database and convert to VCG\n",
        "\n",
        "        PTB database structure: patient folders containing record files\n",
        "        Each record has .dat, .hea, and .xyz files\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory containing PTB patient folders\n",
        "            output_dir: Directory to save VCG signals\n",
        "            max_records: Maximum number of records to process (None for all)\n",
        "        \"\"\"\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Find all patient directories\n",
        "        data_path = Path(data_dir)\n",
        "        patient_dirs = [d for d in data_path.iterdir() if d.is_dir() and d.name.startswith('patient')]\n",
        "\n",
        "        vcg_data = []\n",
        "        processed_count = 0\n",
        "\n",
        "        print(f\"Found {len(patient_dirs)} patient directories\")\n",
        "\n",
        "        for patient_dir in patient_dirs:\n",
        "            # Find all .hea files in this patient directory\n",
        "            hea_files = list(patient_dir.glob(\"*.hea\"))\n",
        "\n",
        "            for hea_file in hea_files:\n",
        "                if max_records and processed_count >= max_records:\n",
        "                    break\n",
        "\n",
        "                record_path = str(hea_file).replace('.hea', '')\n",
        "                record_name = hea_file.stem\n",
        "\n",
        "                print(f\"Processing {processed_count + 1}: {patient_dir.name}/{record_name}\")\n",
        "\n",
        "                signals, fields = self.load_ptb_record(record_path)\n",
        "\n",
        "                if signals is not None and signals.shape[0] > 0:\n",
        "                    lead_names = fields.get('sig_name', [])\n",
        "\n",
        "                    # Convert to VCG\n",
        "                    vcg = self.convert_to_vcg(signals, lead_names)\n",
        "\n",
        "                    # Only keep if we got valid VCG data\n",
        "                    if np.abs(vcg).max() > 0:\n",
        "                        vcg_data.append(vcg)\n",
        "\n",
        "                        # Save individual VCG file\n",
        "                        np.save(f\"{output_dir}/vcg_{patient_dir.name}_{record_name}.npy\", vcg)\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        print(f\"  Warning: Generated VCG has all zeros, skipping\")\n",
        "                else:\n",
        "                    print(f\"  Warning: Could not load signals\")\n",
        "\n",
        "            if max_records and processed_count >= max_records:\n",
        "                break\n",
        "\n",
        "        # Combine all VCG signals\n",
        "        if vcg_data:\n",
        "            all_vcg = np.concatenate(vcg_data, axis=0)\n",
        "            np.save(f\"{output_dir}/all_vcg_signals.npy\", all_vcg)\n",
        "            print(f\"\\nSuccessfully processed {processed_count} records\")\n",
        "            print(f\"Total VCG data shape: {all_vcg.shape}\")\n",
        "            print(f\"Data saved to: {output_dir}\")\n",
        "        else:\n",
        "            print(\"\\nWarning: No VCG data was generated!\")\n",
        "\n",
        "        return vcg_data\n"
      ],
      "metadata": {
        "id": "NuuXfmUuHPFm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VCGPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing for VCG signals:\n",
        "    1. Low-pass Butterworth filter\n",
        "    2. Z-score normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cutoff_freq=40, fs=1000, order=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            cutoff_freq: Cutoff frequency for low-pass filter (Hz)\n",
        "            fs: Sampling frequency (Hz)\n",
        "            order: Filter order\n",
        "        \"\"\"\n",
        "        self.cutoff_freq = cutoff_freq\n",
        "        self.fs = fs\n",
        "        self.order = order\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def butterworth_filter(self, data):\n",
        "        \"\"\"\n",
        "        Apply low-pass Butterworth filter to remove high-frequency noise\n",
        "        \"\"\"\n",
        "        nyquist = 0.5 * self.fs\n",
        "        normal_cutoff = self.cutoff_freq / nyquist\n",
        "        b, a = signal.butter(self.order, normal_cutoff, btype='low', analog=False)\n",
        "\n",
        "        # Apply filter to each channel\n",
        "        filtered_data = np.zeros_like(data)\n",
        "        for i in range(data.shape[1]):\n",
        "            filtered_data[:, i] = signal.filtfilt(b, a, data[:, i])\n",
        "\n",
        "        return filtered_data\n",
        "\n",
        "    def z_score_normalize(self, data, fit=True):\n",
        "        \"\"\"\n",
        "        Z-score normalization: (x - mean) / std\n",
        "\n",
        "        Args:\n",
        "            data: Input data\n",
        "            fit: If True, calculate mean and std; if False, use stored values\n",
        "        \"\"\"\n",
        "        if fit:\n",
        "            self.mean = np.mean(data, axis=0)\n",
        "            self.std = np.std(data, axis=0)\n",
        "\n",
        "        normalized = (data - self.mean) / (self.std + 1e-8)\n",
        "        return normalized\n",
        "\n",
        "    def inverse_normalize(self, normalized_data):\n",
        "        \"\"\"\n",
        "        Inverse Z-score normalization\n",
        "        \"\"\"\n",
        "        return normalized_data * self.std + self.mean\n",
        "\n",
        "    def preprocess(self, data, fit=True):\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline\n",
        "        \"\"\"\n",
        "        # Step 1: Butterworth filter\n",
        "        filtered = self.butterworth_filter(data)\n",
        "\n",
        "        # Step 2: Z-score normalization\n",
        "        normalized = self.z_score_normalize(filtered, fit=fit)\n",
        "\n",
        "        return normalized"
      ],
      "metadata": {
        "id": "ss15tPSWHV0D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VCGDataGenerator:\n",
        "    \"\"\"\n",
        "    Prepare VCG data for autoencoder training\n",
        "    Creates sliding windows from continuous signals\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=1250, overlap=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            window_size: Number of samples per window (1250 ≈ 1.25s at 1000Hz)\n",
        "            overlap: Overlap ratio between consecutive windows\n",
        "        \"\"\"\n",
        "        self.window_size = window_size\n",
        "        self.stride = int(window_size * (1 - overlap))\n",
        "\n",
        "    def create_windows(self, vcg_data):\n",
        "        \"\"\"\n",
        "        Create sliding windows from VCG signal\n",
        "\n",
        "        Args:\n",
        "            vcg_data: Array of shape (samples, 3) for X, Y, Z\n",
        "\n",
        "        Returns:\n",
        "            windows: Array of shape (n_windows, window_size, 3)\n",
        "        \"\"\"\n",
        "        n_samples = vcg_data.shape[0]\n",
        "        n_windows = (n_samples - self.window_size) // self.stride + 1\n",
        "\n",
        "        windows = np.zeros((n_windows, self.window_size, 3))\n",
        "\n",
        "        for i in range(n_windows):\n",
        "            start_idx = i * self.stride\n",
        "            end_idx = start_idx + self.window_size\n",
        "            windows[i] = vcg_data[start_idx:end_idx]\n",
        "\n",
        "        return windows\n",
        "\n",
        "    def prepare_dataset(self, vcg_files, preprocessor, test_size=0.15):\n",
        "        \"\"\"\n",
        "        Prepare complete dataset for training\n",
        "\n",
        "        Args:\n",
        "            vcg_files: List of VCG .npy files\n",
        "            preprocessor: VCGPreprocessor instance\n",
        "            test_size: Fraction of data for testing\n",
        "\n",
        "        Returns:\n",
        "            X_train, X_test: Training and test sets\n",
        "        \"\"\"\n",
        "        all_windows = []\n",
        "\n",
        "        for vcg_file in vcg_files:\n",
        "            # Load VCG data\n",
        "            vcg_data = np.load(vcg_file)\n",
        "\n",
        "            # Preprocess\n",
        "            preprocessed = preprocessor.preprocess(vcg_data, fit=True)\n",
        "\n",
        "            # Create windows\n",
        "            windows = self.create_windows(preprocessed)\n",
        "            all_windows.append(windows)\n",
        "\n",
        "        # Concatenate all windows\n",
        "        all_windows = np.concatenate(all_windows, axis=0)\n",
        "\n",
        "        # Split into train/test\n",
        "        X_train, X_test = train_test_split(\n",
        "            all_windows,\n",
        "            test_size=test_size,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Training samples: {X_train.shape[0]}\")\n",
        "        print(f\"Test samples: {X_test.shape[0]}\")\n",
        "        print(f\"Window shape: {X_train.shape[1:]}\")\n",
        "\n",
        "        return X_train, X_test\n",
        "\n"
      ],
      "metadata": {
        "id": "_qn6MkcUHa62"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VCGAutoencoder:\n",
        "    \"\"\"\n",
        "    CNN-LSTM Autoencoder for VCG signal compression\n",
        "    Architecture based on the research methodology\n",
        "\n",
        "    Encoder: Conv1D blocks → LSTM layers → Dense (bottleneck)\n",
        "    Decoder: Dense → LSTM layers → Conv1DTranspose blocks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(1250, 3), compression_ratio=30):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_shape: Shape of input VCG window (samples, channels)\n",
        "            compression_ratio: Target compression ratio\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.compression_ratio = compression_ratio\n",
        "\n",
        "        # Calculate bottleneck size based on compression ratio\n",
        "        self.bottleneck_size = int(\n",
        "            (input_shape[0] * input_shape[1]) / compression_ratio\n",
        "        )\n",
        "\n",
        "        self.model = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "    def build_encoder(self):\n",
        "        \"\"\"\n",
        "        Build encoder network: Conv1D blocks → LSTM → Dense\n",
        "\n",
        "        Architecture from methodology:\n",
        "        - 3 Conv1D blocks (32, 64, 128 filters)\n",
        "        - 3 LSTM layers (128, 128, 64 units) with dropout\n",
        "        - Dense bottleneck layer\n",
        "        \"\"\"\n",
        "        inputs = layers.Input(shape=self.input_shape, name='encoder_input')\n",
        "\n",
        "        # Convolutional Block 1: 32 filters, kernel=5\n",
        "        x = layers.Conv1D(32, kernel_size=5, padding='same',\n",
        "                         name='encoder_conv1')(inputs)\n",
        "        x = layers.BatchNormalization(name='encoder_bn1')(x)\n",
        "        x = layers.ReLU(name='encoder_relu1')(x)\n",
        "        x = layers.MaxPooling1D(pool_size=2, name='encoder_pool1')(x)  # 1250 → 625\n",
        "\n",
        "        # Convolutional Block 2: 64 filters, kernel=5\n",
        "        x = layers.Conv1D(64, kernel_size=5, padding='same',\n",
        "                         name='encoder_conv2')(x)\n",
        "        x = layers.BatchNormalization(name='encoder_bn2')(x)\n",
        "        x = layers.ReLU(name='encoder_relu2')(x)\n",
        "        x = layers.MaxPooling1D(pool_size=2, name='encoder_pool2')(x)  # 625 → 312\n",
        "\n",
        "        # Convolutional Block 3: 128 filters, kernel=5\n",
        "        x = layers.Conv1D(128, kernel_size=5, padding='same',\n",
        "                         name='encoder_conv3')(x)\n",
        "        x = layers.BatchNormalization(name='encoder_bn3')(x)\n",
        "        x = layers.ReLU(name='encoder_relu3')(x)\n",
        "        x = layers.MaxPooling1D(pool_size=2, name='encoder_pool3')(x)  # 312 → 156\n",
        "\n",
        "        # LSTM layers for temporal dependencies\n",
        "        # LSTM 1: 128 units\n",
        "        x = layers.LSTM(128, return_sequences=True, dropout=0.2,\n",
        "                       name='encoder_lstm1')(x)\n",
        "        x = layers.BatchNormalization(name='encoder_lstm_bn1')(x)\n",
        "\n",
        "        # LSTM 2: 128 units\n",
        "        x = layers.LSTM(128, return_sequences=True, dropout=0.2,\n",
        "                       name='encoder_lstm2')(x)\n",
        "        x = layers.BatchNormalization(name='encoder_lstm_bn2')(x)\n",
        "\n",
        "        # LSTM 3: 64 units (final sequence layer)\n",
        "        x = layers.LSTM(64, return_sequences=False, dropout=0.2,\n",
        "                       name='encoder_lstm3')(x)\n",
        "        x = layers.BatchNormalization(name='encoder_lstm_bn3')(x)\n",
        "\n",
        "        # Flatten and create bottleneck (compressed representation)\n",
        "        x = layers.Dense(self.bottleneck_size,\n",
        "                        activation='relu',\n",
        "                        name='bottleneck')(x)\n",
        "\n",
        "        encoder = models.Model(inputs, x, name='encoder')\n",
        "        return encoder\n",
        "\n",
        "    def build_decoder(self):\n",
        "        \"\"\"\n",
        "        Build decoder network: Dense → LSTM → Conv1DTranspose\n",
        "\n",
        "        Mirrors the encoder architecture to reconstruct the signal\n",
        "        \"\"\"\n",
        "        # Calculate intermediate shape after encoding\n",
        "        # After 3 MaxPooling layers: 1250 -> 625 -> 312 -> 156\n",
        "        intermediate_length = 156  # Explicit calculation\n",
        "        intermediate_features = 64  # Last LSTM output\n",
        "\n",
        "        inputs = layers.Input(shape=(self.bottleneck_size,), name='decoder_input')\n",
        "\n",
        "        # Dense layer to expand bottleneck\n",
        "        x = layers.Dense(intermediate_length * intermediate_features,\n",
        "                        activation='relu',\n",
        "                        name='decoder_dense')(inputs)\n",
        "        x = layers.Reshape((intermediate_length, intermediate_features),\n",
        "                          name='decoder_reshape')(x)\n",
        "\n",
        "        # LSTM layers (reverse of encoder)\n",
        "        # LSTM 1: 64 units\n",
        "        x = layers.LSTM(64, return_sequences=True, dropout=0.2,\n",
        "                       name='decoder_lstm1')(x)\n",
        "        x = layers.BatchNormalization(name='decoder_lstm_bn1')(x)\n",
        "\n",
        "        # LSTM 2: 128 units\n",
        "        x = layers.LSTM(128, return_sequences=True, dropout=0.2,\n",
        "                       name='decoder_lstm2')(x)\n",
        "        x = layers.BatchNormalization(name='decoder_lstm_bn2')(x)\n",
        "\n",
        "        # LSTM 3: 128 units\n",
        "        x = layers.LSTM(128, return_sequences=True, dropout=0.2,\n",
        "                       name='decoder_lstm3')(x)\n",
        "        x = layers.BatchNormalization(name='decoder_lstm_bn3')(x)\n",
        "\n",
        "        # Conv1DTranspose blocks (reverse of encoder)\n",
        "        # Upsampling 1: 156 → 312\n",
        "        x = layers.UpSampling1D(size=2, name='decoder_upsample1')(x)\n",
        "        x = layers.Conv1DTranspose(128, kernel_size=5, padding='same',\n",
        "                                   name='decoder_conv1')(x)\n",
        "        x = layers.BatchNormalization(name='decoder_bn1')(x)\n",
        "        x = layers.ReLU(name='decoder_relu1')(x)\n",
        "\n",
        "        # Upsampling 2: 312 → 624\n",
        "        x = layers.UpSampling1D(size=2, name='decoder_upsample2')(x)\n",
        "        x = layers.Conv1DTranspose(64, kernel_size=5, padding='same',\n",
        "                                   name='decoder_conv2')(x)\n",
        "        x = layers.BatchNormalization(name='decoder_bn2')(x)\n",
        "        x = layers.ReLU(name='decoder_relu2')(x)\n",
        "\n",
        "        # Upsampling 3: 624 → 1248\n",
        "        x = layers.UpSampling1D(size=2, name='decoder_upsample3')(x)\n",
        "        x = layers.Conv1DTranspose(32, kernel_size=5, padding='same',\n",
        "                                   name='decoder_conv3')(x)\n",
        "        x = layers.BatchNormalization(name='decoder_bn3')(x)\n",
        "        x = layers.ReLU(name='decoder_relu3')(x)\n",
        "\n",
        "        # Now at 1248, need to get to 1250\n",
        "        # Use ZeroPadding1D to add 2 samples (1 on each side)\n",
        "        x = layers.ZeroPadding1D(padding=(1, 1), name='decoder_padding')(x)  # 1248 → 1250\n",
        "\n",
        "        # Final layer to get 3 channels (X, Y, Z)\n",
        "        x = layers.Conv1D(self.input_shape[1], kernel_size=3,\n",
        "                         padding='same',\n",
        "                         activation='linear',\n",
        "                         name='decoder_output')(x)\n",
        "\n",
        "        decoder = models.Model(inputs, x, name='decoder')\n",
        "        return decoder\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build complete autoencoder by connecting encoder and decoder\n",
        "        \"\"\"\n",
        "        self.encoder = self.build_encoder()\n",
        "        self.decoder = self.build_decoder()\n",
        "\n",
        "        # Connect encoder and decoder\n",
        "        inputs = layers.Input(shape=self.input_shape, name='autoencoder_input')\n",
        "        encoded = self.encoder(inputs)\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        self.model = models.Model(inputs, decoded, name='vcg_autoencoder')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ENCODER ARCHITECTURE\")\n",
        "        print(\"=\"*70)\n",
        "        self.encoder.summary()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DECODER ARCHITECTURE\")\n",
        "        print(\"=\"*70)\n",
        "        self.decoder.summary()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"COMPLETE AUTOENCODER\")\n",
        "        print(\"=\"*70)\n",
        "        self.model.summary()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        Compile model with optimizer and loss function\n",
        "        Using Mean Squared Error (MSE) as per the paper\n",
        "        \"\"\"\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='mse',  # Mean Squared Error\n",
        "            metrics=['mae']  # Mean Absolute Error for monitoring\n",
        "        )\n",
        "\n",
        "        print(f\"\\nModel compiled with:\")\n",
        "        print(f\"  - Optimizer: Adam (lr={learning_rate})\")\n",
        "        print(f\"  - Loss: MSE\")\n",
        "        print(f\"  - Bottleneck size: {self.bottleneck_size}\")\n",
        "        print(f\"  - Compression ratio: ~{self.compression_ratio}:1\")\n",
        "\n",
        "    def train(self, X_train, X_test, epochs=120, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the autoencoder model\n",
        "\n",
        "        Args:\n",
        "            X_train: Training data\n",
        "            X_test: Validation data\n",
        "            epochs: Number of training epochs (paper uses 120)\n",
        "            batch_size: Batch size for training\n",
        "        \"\"\"\n",
        "        # Check if we have enough data\n",
        "        if X_train.shape[0] < 100:\n",
        "            print(f\"\\n⚠️  WARNING: Very small training set ({X_train.shape[0]} samples)\")\n",
        "            print(\"    Reducing epochs and adjusting learning rate for stability\")\n",
        "            epochs = min(epochs, 50)\n",
        "            learning_rate = 0.0005\n",
        "            self.model.optimizer.learning_rate.assign(learning_rate)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=20,  # Increased patience\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=10,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                'best_vcg_autoencoder.keras',  # Use .keras format\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nStarting training...\")\n",
        "        print(f\"  - Training samples: {X_train.shape[0]}\")\n",
        "        print(f\"  - Validation samples: {X_test.shape[0]}\")\n",
        "        print(f\"  - Epochs: {epochs}\")\n",
        "        print(f\"  - Batch size: {batch_size}\")\n",
        "        print(f\"  - Input range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train, X_train,  # Autoencoder: input = target\n",
        "            validation_data=(X_test, X_test),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def compress(self, vcg_signal):\n",
        "        \"\"\"\n",
        "        Compress VCG signal using encoder\n",
        "\n",
        "        Args:\n",
        "            vcg_signal: Input VCG signal (batch, samples, channels)\n",
        "\n",
        "        Returns:\n",
        "            compressed: Compressed representation\n",
        "        \"\"\"\n",
        "        return self.encoder.predict(vcg_signal, verbose=0)\n",
        "\n",
        "    def decompress(self, compressed_signal):\n",
        "        \"\"\"\n",
        "        Decompress VCG signal using decoder\n",
        "\n",
        "        Args:\n",
        "            compressed_signal: Compressed representation\n",
        "\n",
        "        Returns:\n",
        "            reconstructed: Reconstructed VCG signal\n",
        "        \"\"\"\n",
        "        return self.decoder.predict(compressed_signal, verbose=0)\n",
        "\n",
        "    def save_models(self, encoder_path='encoder.h5', decoder_path='decoder.h5'):\n",
        "        \"\"\"Save encoder and decoder separately for deployment\"\"\"\n",
        "        self.encoder.save(encoder_path)\n",
        "        self.decoder.save(decoder_path)\n",
        "        print(f\"Models saved: {encoder_path}, {decoder_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xhvISejaHgwe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PerformanceEvaluator:\n",
        "    \"\"\"\n",
        "    Calculate all performance metrics as described in the paper:\n",
        "    - Compression Ratio (CR)\n",
        "    - Mean Squared Error (MSE)\n",
        "    - Root Mean Square Error (RMSE)\n",
        "    - Normalized MSE (NMSE)\n",
        "    - Percentage RMS Difference (PRD)\n",
        "    - PRD Normalized (PRDN)\n",
        "    - Signal-to-Noise Ratio (SNR)\n",
        "    - Peak Signal-to-Noise Ratio (PSNR)\n",
        "    - Quality Score (QS)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def compression_ratio(original_size, compressed_size):\n",
        "        \"\"\"\n",
        "        CR = Original Size / Compressed Size\n",
        "        \"\"\"\n",
        "        return original_size / compressed_size\n",
        "\n",
        "    @staticmethod\n",
        "    def mse(original, reconstructed):\n",
        "        \"\"\"\n",
        "        MSE = (1/n) * Σ(xi - x̂i)²\n",
        "        \"\"\"\n",
        "        return np.mean((original - reconstructed) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rmse(original, reconstructed):\n",
        "        \"\"\"\n",
        "        RMSE = √(MSE)\n",
        "        \"\"\"\n",
        "        return np.sqrt(PerformanceEvaluator.mse(original, reconstructed))\n",
        "\n",
        "    @staticmethod\n",
        "    def nmse(original, reconstructed):\n",
        "        \"\"\"\n",
        "        NMSE = Σ(xi - x̂i)² / Σ(xi - x̄)²\n",
        "        \"\"\"\n",
        "        numerator = np.sum((original - reconstructed) ** 2)\n",
        "        denominator = np.sum((original - np.mean(original)) ** 2)\n",
        "        return numerator / (denominator + 1e-10)\n",
        "\n",
        "    @staticmethod\n",
        "    def prd(original, reconstructed):\n",
        "        \"\"\"\n",
        "        PRD = √(Σ(xi - x̂i)² / Σ(xi)²) * 100\n",
        "        \"\"\"\n",
        "        numerator = np.sum((original - reconstructed) ** 2)\n",
        "        denominator = np.sum(original ** 2)\n",
        "        return np.sqrt(numerator / (denominator + 1e-10)) * 100\n",
        "\n",
        "    @staticmethod\n",
        "    def prdn(original, reconstructed):\n",
        "        \"\"\"\n",
        "        PRDN = PRD * Range / 100\n",
        "        where Range = max(xi) - min(xi)\n",
        "        \"\"\"\n",
        "        prd_value = PerformanceEvaluator.prd(original, reconstructed)\n",
        "        signal_range = np.max(original) - np.min(original)\n",
        "        return (prd_value * signal_range) / 100\n",
        "\n",
        "    @staticmethod\n",
        "    def snr(original, reconstructed):\n",
        "        \"\"\"\n",
        "        SNR = 10 * log10(Signal Power / Noise Power)\n",
        "        \"\"\"\n",
        "        signal_power = np.sum(original ** 2)\n",
        "        noise_power = np.sum((original - reconstructed) ** 2)\n",
        "        return 10 * np.log10(signal_power / (noise_power + 1e-10))\n",
        "\n",
        "    @staticmethod\n",
        "    def psnr(original, reconstructed):\n",
        "        \"\"\"\n",
        "        PSNR = 10 * log10(MAX² / MSE)\n",
        "        \"\"\"\n",
        "        max_value = np.max(np.abs(original))\n",
        "        mse_value = PerformanceEvaluator.mse(original, reconstructed)\n",
        "        return 10 * np.log10((max_value ** 2) / (mse_value + 1e-10))\n",
        "\n",
        "    @staticmethod\n",
        "    def quality_score(cr, prd):\n",
        "        \"\"\"\n",
        "        QS = CR / PRD\n",
        "        Higher QS indicates better compression quality\n",
        "        \"\"\"\n",
        "        return cr / (prd + 1e-10)\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_all(original, reconstructed, compressed_size):\n",
        "        \"\"\"\n",
        "        Calculate all metrics and return as dictionary\n",
        "\n",
        "        Args:\n",
        "            original: Original VCG signal\n",
        "            reconstructed: Reconstructed VCG signal\n",
        "            compressed_size: Size of compressed representation\n",
        "\n",
        "        Returns:\n",
        "            metrics: Dictionary of all performance metrics\n",
        "        \"\"\"\n",
        "        original_size = original.size\n",
        "\n",
        "        cr = PerformanceEvaluator.compression_ratio(original_size, compressed_size)\n",
        "        mse = PerformanceEvaluator.mse(original, reconstructed)\n",
        "        rmse = PerformanceEvaluator.rmse(original, reconstructed)\n",
        "        nmse = PerformanceEvaluator.nmse(original, reconstructed)\n",
        "        prd = PerformanceEvaluator.prd(original, reconstructed)\n",
        "        prdn = PerformanceEvaluator.prdn(original, reconstructed)\n",
        "        snr = PerformanceEvaluator.snr(original, reconstructed)\n",
        "        psnr = PerformanceEvaluator.psnr(original, reconstructed)\n",
        "        qs = PerformanceEvaluator.quality_score(cr, prd)\n",
        "\n",
        "        metrics = {\n",
        "            'CR': cr,\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'NMSE': nmse,\n",
        "            'PRD': prd,\n",
        "            'PRDN': prdn,\n",
        "            'SNR': snr,\n",
        "            'PSNR': psnr,\n",
        "            'QS': qs\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def print_metrics(metrics):\n",
        "        \"\"\"Pretty print all metrics\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PERFORMANCE METRICS\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Compression Ratio (CR):           {metrics['CR']:.2f}\")\n",
        "        print(f\"Mean Squared Error (MSE):         {metrics['MSE']:.6f}\")\n",
        "        print(f\"Root Mean Square Error (RMSE):    {metrics['RMSE']:.4f}\")\n",
        "        print(f\"Normalized MSE (NMSE):            {metrics['NMSE']:.6f}\")\n",
        "        print(f\"Percentage RMS Difference (PRD):  {metrics['PRD']:.2f}%\")\n",
        "        print(f\"PRD Normalized (PRDN):            {metrics['PRDN']:.2f}\")\n",
        "        print(f\"Signal-to-Noise Ratio (SNR):      {metrics['SNR']:.2f} dB\")\n",
        "        print(f\"Peak SNR (PSNR):                  {metrics['PSNR']:.2f} dB\")\n",
        "        print(f\"Quality Score (QS):               {metrics['QS']:.2f}\")\n",
        "        print(\"=\"*70)\n",
        "\n"
      ],
      "metadata": {
        "id": "2gHUBtMeHiGc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation loss\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Training MAE')\n",
        "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.title('Mean Absolute Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"  ✓ Saved: training_history.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_vcg_comparison(original, reconstructed, sample_idx=0):\n",
        "    \"\"\"\n",
        "    Plot original vs reconstructed VCG signals for all 3 channels\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
        "    channels = ['X (Frontal)', 'Y (Sagittal)', 'Z (Horizontal)']\n",
        "\n",
        "    for i, (ax, channel) in enumerate(zip(axes, channels)):\n",
        "        time_axis = np.arange(len(original[sample_idx, :, i])) / 1000  # Convert to seconds\n",
        "\n",
        "        ax.plot(time_axis, original[sample_idx, :, i],\n",
        "                label='Original', linewidth=1.5, alpha=0.7)\n",
        "        ax.plot(time_axis, reconstructed[sample_idx, :, i],\n",
        "                label='Reconstructed', linewidth=1.5, alpha=0.7, linestyle='--')\n",
        "\n",
        "        ax.set_xlabel('Time (s)')\n",
        "        ax.set_ylabel('Amplitude')\n",
        "        ax.set_title(f'VCG Channel {channel}')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vcg_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"  ✓ Saved: vcg_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_3d_vcg(vcg_signal, title='VCG Signal', filename='vcg_3d_trajectory.png'):\n",
        "    \"\"\"\n",
        "    Plot 3D vectorcardiogram (X, Y, Z trajectory)\n",
        "    \"\"\"\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot VCG trajectory\n",
        "    ax.plot(vcg_signal[:, 0], vcg_signal[:, 1], vcg_signal[:, 2],\n",
        "            linewidth=1.5, alpha=0.7)\n",
        "\n",
        "    # Mark start and end points\n",
        "    ax.scatter(vcg_signal[0, 0], vcg_signal[0, 1], vcg_signal[0, 2],\n",
        "               c='green', s=100, label='Start', marker='o')\n",
        "    ax.scatter(vcg_signal[-1, 0], vcg_signal[-1, 1], vcg_signal[-1, 2],\n",
        "               c='red', s=100, label='End', marker='x')\n",
        "\n",
        "    ax.set_xlabel('X (Frontal)')\n",
        "    ax.set_ylabel('Y (Sagittal)')\n",
        "    ax.set_zlabel('Z (Horizontal)')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    print(f\"  ✓ Saved: {filename}\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "crGD8iVkJh2Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Complete pipeline for VCG signal compression\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"VCG SIGNAL COMPRESSION USING CNN-LSTM AUTOENCODER\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ========== STEP 1: ECG to VCG Conversion ==========\n",
        "    print(\"\\n[STEP 1] Converting ECG to VCG...\")\n",
        "    converter = ECGtoVCGConverter()\n",
        "\n",
        "    # IMPORTANT: Set your PTB database path here\n",
        "    data_dir = \"/content/drive/MyDrive/ptb-diagnostic-ecg-database-1.0.0\"\n",
        "    #data_dir = \"./ptb-diagnostic-ecg-database-1.0.0\"  # Update this path!\n",
        "    output_dir = \"vcg_data\"\n",
        "\n",
        "    # Check if data directory exists\n",
        "    if not Path(data_dir).exists():\n",
        "        print(f\"\\n⚠️  ERROR: Data directory not found: {data_dir}\")\n",
        "        print(\"\\nPlease download the PTB Diagnostic ECG Database:\")\n",
        "        print(\"1. Visit: https://www.physionet.org/content/ptbdb/1.0.0/\")\n",
        "        print(\"2. Download and extract the database\")\n",
        "        print(\"3. Update the 'data_dir' variable in main() function\")\n",
        "        print(\"\\nFor now, creating a LARGER synthetic dataset for demonstration...\")\n",
        "        print(\"Note: Results will be poor with synthetic data!\")\n",
        "\n",
        "        # Create more realistic synthetic VCG data\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Generate multiple synthetic records with more data\n",
        "        for i in range(10):  # Create 10 synthetic records\n",
        "            # Create synthetic VCG with some cardiac-like patterns\n",
        "            n_samples = 50000  # 50 seconds at 1000 Hz\n",
        "            t = np.linspace(0, 50, n_samples)\n",
        "\n",
        "            # Simulate heartbeat-like patterns (rough approximation)\n",
        "            heart_rate = 70  # bpm\n",
        "            num_beats = int(50 * heart_rate / 60)\n",
        "\n",
        "            vcg = np.zeros((n_samples, 3))\n",
        "            for beat in range(num_beats):\n",
        "                beat_center = int((beat / num_beats) * n_samples)\n",
        "                beat_width = int(0.2 * 1000)  # 200ms beat width\n",
        "\n",
        "                # Create a simple QRS-like complex for each channel\n",
        "                start_idx = max(0, beat_center - beat_width // 2)\n",
        "                end_idx = min(n_samples, beat_center + beat_width // 2)\n",
        "                beat_len = end_idx - start_idx\n",
        "\n",
        "                if beat_len > 0:\n",
        "                    # X channel\n",
        "                    vcg[start_idx:end_idx, 0] += np.sin(np.linspace(0, 2*np.pi, beat_len)) * 1.5\n",
        "                    # Y channel\n",
        "                    vcg[start_idx:end_idx, 1] += np.sin(np.linspace(0, 2*np.pi, beat_len)) * 1.2\n",
        "                    # Z channel\n",
        "                    vcg[start_idx:end_idx, 2] += np.sin(np.linspace(0, 2*np.pi, beat_len)) * 0.8\n",
        "\n",
        "            # Add some noise\n",
        "            vcg += np.random.randn(n_samples, 3) * 0.05\n",
        "\n",
        "            np.save(f\"{output_dir}/synthetic_vcg_{i}.npy\", vcg)\n",
        "\n",
        "        print(f\"  Created 10 synthetic VCG files in {output_dir}/\")\n",
        "    else:\n",
        "        # Process real PTB database\n",
        "        print(f\"  Found PTB database at: {data_dir}\")\n",
        "        print(\"  Processing records...\")\n",
        "        vcg_data = converter.process_ptb_database(\n",
        "            data_dir,\n",
        "            output_dir,\n",
        "            max_records=50  # Process up to 50 records\n",
        "        )\n",
        "\n",
        "        if not vcg_data:\n",
        "            print(\"\\n⚠️  ERROR: No VCG data was generated from PTB database!\")\n",
        "            print(\"Please check if the database path is correct.\")\n",
        "            return None, None\n",
        "\n",
        "    # ========== STEP 2: Preprocessing ==========\n",
        "    print(\"\\n[STEP 2] Preprocessing VCG signals...\")\n",
        "    preprocessor = VCGPreprocessor(cutoff_freq=40, fs=1000, order=5)\n",
        "\n",
        "    # ========== STEP 3: Prepare Dataset ==========\n",
        "    print(\"\\n[STEP 3] Preparing training dataset...\")\n",
        "    data_generator = VCGDataGenerator(window_size=1250, overlap=0.5)\n",
        "\n",
        "    # Get list of VCG files\n",
        "    vcg_files = list(Path(output_dir).glob(\"*.npy\"))\n",
        "    vcg_files = [f for f in vcg_files if 'all_vcg' not in f.name]  # Exclude combined file\n",
        "    print(f\"  Found {len(vcg_files)} VCG files\")\n",
        "\n",
        "    if len(vcg_files) == 0:\n",
        "        print(\"\\n⚠️  ERROR: No VCG files found!\")\n",
        "        return None, None\n",
        "\n",
        "    # Prepare train/test split\n",
        "    X_train, X_test = data_generator.prepare_dataset(\n",
        "        vcg_files,\n",
        "        preprocessor,\n",
        "        test_size=0.15\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  Dataset Statistics:\")\n",
        "    print(f\"    Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"    Test samples: {X_test.shape[0]}\")\n",
        "    print(f\"    Signal range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
        "    print(f\"    Signal std: {X_train.std():.3f}\")\n",
        "\n",
        "    # ========== STEP 4: Build and Train Model ==========\n",
        "    print(\"\\n[STEP 4] Building CNN-LSTM Autoencoder...\")\n",
        "    autoencoder = VCGAutoencoder(\n",
        "        input_shape=(1250, 3),\n",
        "        compression_ratio=30  # Target: 30:1 compression\n",
        "    )\n",
        "\n",
        "    autoencoder.build_model()\n",
        "    autoencoder.compile_model(learning_rate=0.001)\n",
        "\n",
        "    print(\"\\n[STEP 5] Training model...\")\n",
        "    history = autoencoder.train(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        epochs=120,  # Use fewer epochs if testing\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    # ========== STEP 6: Evaluate Performance ==========\n",
        "    print(\"\\n[STEP 6] Evaluating performance...\")\n",
        "\n",
        "    # Get predictions on test set\n",
        "    X_test_reconstructed = autoencoder.model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Get compressed representation\n",
        "    compressed = autoencoder.compress(X_test)\n",
        "\n",
        "    # Calculate metrics for each test sample\n",
        "    all_metrics = []\n",
        "    for i in range(min(10, len(X_test))):  # Evaluate first 10 samples\n",
        "        metrics = PerformanceEvaluator.evaluate_all(\n",
        "            original=X_test[i],\n",
        "            reconstructed=X_test_reconstructed[i],\n",
        "            compressed_size=compressed[i].size\n",
        "        )\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "    # Average metrics\n",
        "    avg_metrics = {\n",
        "        key: np.mean([m[key] for m in all_metrics])\n",
        "        for key in all_metrics[0].keys()\n",
        "    }\n",
        "\n",
        "    print(\"\\n[AVERAGE PERFORMANCE ACROSS TEST SAMPLES]\")\n",
        "    PerformanceEvaluator.print_metrics(avg_metrics)\n",
        "\n",
        "    # ========== STEP 7: Visualizations ==========\n",
        "    print(\"\\n[STEP 7] Creating visualizations...\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Plot signal comparison\n",
        "    plot_vcg_comparison(X_test, X_test_reconstructed, sample_idx=0)\n",
        "\n",
        "    # Plot 3D VCG trajectory\n",
        "    plot_3d_vcg(X_test[0], title='Original VCG Signal',\n",
        "                filename='vcg_3d_original.png')\n",
        "    plot_3d_vcg(X_test_reconstructed[0], title='Reconstructed VCG Signal',\n",
        "                filename='vcg_3d_reconstructed.png')\n",
        "\n",
        "    # ========== STEP 8: Save Models ==========\n",
        "    print(\"\\n[STEP 8] Saving models...\")\n",
        "    autoencoder.save_models('vcg_encoder.keras', 'vcg_decoder.keras')\n",
        "\n",
        "    # ========== STEP 9: Raspberry Pi Optimization ==========\n",
        "    # print(\"\\n[STEP 9] Optimizing for Raspberry Pi deployment...\")\n",
        "    # optimize_for_raspberry_pi('best_vcg_autoencoder.keras',\n",
        "    #                           'vcg_autoencoder_pi.tflite')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  - best_vcg_autoencoder.keras (Complete model)\")\n",
        "    print(\"  - vcg_encoder.keras (Encoder only)\")\n",
        "    print(\"  - vcg_decoder.keras (Decoder only)\")\n",
        "    print(\"  - vcg_autoencoder_pi.tflite (Raspberry Pi optimized)\")\n",
        "    print(\"\\nVisualization files:\")\n",
        "    print(\"  - training_history.png\")\n",
        "    print(\"  - vcg_comparison.png\")\n",
        "    print(\"  - vcg_3d_original.png\")\n",
        "    print(\"  - vcg_3d_reconstructed.png\")\n",
        "    print(\"\\nTo view visualizations, open the PNG files in your file explorer.\")\n",
        "    print(\"\\n📊 Performance Summary:\")\n",
        "    print(f\"    Compression Ratio: {avg_metrics['CR']:.2f}:1\")\n",
        "    print(f\"    PRD: {avg_metrics['PRD']:.2f}%\")\n",
        "    print(f\"    SNR: {avg_metrics['SNR']:.2f} dB\")\n",
        "    print(f\"    Quality Score: {avg_metrics['QS']:.2f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return autoencoder, avg_metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "2DghKSiAHmxq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    autoencoder, metrics = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2LH-tllPHx87",
        "outputId": "4aaf2829-745f-49da-a044-245eb7637eba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VCG SIGNAL COMPRESSION USING CNN-LSTM AUTOENCODER\n",
            "======================================================================\n",
            "\n",
            "[STEP 1] Converting ECG to VCG...\n",
            "  Found PTB database at: /content/drive/MyDrive/ptb-diagnostic-ecg-database-1.0.0\n",
            "  Processing records...\n",
            "Found 290 patient directories\n",
            "Processing 1: patient001/s0016lre\n",
            "Processing 2: patient001/s0014lre\n",
            "Processing 3: patient001/s0010_re\n",
            "Processing 4: patient002/s0015lre\n",
            "Processing 5: patient004/s0020are\n",
            "Processing 6: patient004/s0020bre\n",
            "Processing 7: patient003/s0017lre\n",
            "Processing 8: patient006/s0027lre\n",
            "Processing 9: patient006/s0064lre\n",
            "Processing 10: patient006/s0022lre\n",
            "Processing 11: patient009/s0035_re\n",
            "Processing 12: patient007/s0029lre\n",
            "Processing 13: patient007/s0026lre\n",
            "Processing 14: patient007/s0038lre\n",
            "Processing 15: patient007/s0078lre\n",
            "Processing 16: patient008/s0037lre\n",
            "Processing 17: patient008/s0068lre\n",
            "Processing 18: patient008/s0028lre\n",
            "Processing 19: patient005/s0021bre\n",
            "Processing 20: patient005/s0101lre\n",
            "Processing 21: patient005/s0031lre\n",
            "Processing 22: patient005/s0021are\n",
            "Processing 23: patient005/s0025lre\n",
            "Processing 24: patient010/s0042lre\n",
            "Processing 25: patient010/s0036lre\n",
            "Processing 26: patient010/s0061lre\n",
            "Processing 27: patient016/s0076lre\n",
            "Processing 28: patient016/s0052lre\n",
            "Processing 29: patient016/s0060lre\n",
            "Processing 30: patient011/s0044lre\n",
            "Processing 31: patient011/s0049lre\n",
            "Processing 32: patient011/s0067lre\n",
            "Processing 33: patient011/s0039lre\n",
            "Processing 34: patient012/s0050lre\n",
            "Processing 35: patient012/s0043lre\n",
            "Processing 36: patient013/s0045lre\n",
            "Processing 37: patient013/s0072lre\n",
            "Processing 38: patient013/s0051lre\n",
            "Processing 39: patient015/s0152lre\n",
            "Processing 40: patient015/s0057lre\n",
            "Processing 41: patient015/s0047lre\n",
            "Processing 42: patient014/s0071lre\n",
            "Processing 43: patient014/s0056lre\n",
            "Processing 44: patient014/s0046lre\n",
            "Processing 45: patient019/s0070lre\n",
            "Processing 46: patient019/s0058lre\n",
            "Processing 47: patient019/s0077lre\n",
            "Processing 48: patient017/s0053lre\n",
            "Processing 49: patient017/s0055lre\n",
            "Processing 50: patient017/s0063lre\n",
            "\n",
            "Successfully processed 50 records\n",
            "Total VCG data shape: (5516722, 3)\n",
            "Data saved to: vcg_data\n",
            "\n",
            "[STEP 2] Preprocessing VCG signals...\n",
            "\n",
            "[STEP 3] Preparing training dataset...\n",
            "  Found 50 VCG files\n",
            "Training samples: 7446\n",
            "Test samples: 1315\n",
            "Window shape: (1250, 3)\n",
            "\n",
            "  Dataset Statistics:\n",
            "    Training samples: 7446\n",
            "    Test samples: 1315\n",
            "    Signal range: [-7.842, 8.414]\n",
            "    Signal std: 0.997\n",
            "\n",
            "[STEP 4] Building CNN-LSTM Autoencoder...\n",
            "\n",
            "======================================================================\n",
            "ENCODER ARCHITECTURE\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"encoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m3\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_conv1 (\u001b[38;5;33mConv1D\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_bn1                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_relu1 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_pool1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_conv2 (\u001b[38;5;33mConv1D\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m10,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_bn2                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_relu2 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_pool2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_conv3 (\u001b[38;5;33mConv1D\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m41,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_bn3                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_relu3 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_pool3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm1 (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm_bn1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm2 (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm_bn2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm3 (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm_bn3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bottleneck (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m)            │         \u001b[38;5;34m8,125\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_bn1                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_bn2                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_bn3                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_relu3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_pool3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm_bn1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm_bn2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm_bn3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bottleneck (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,125</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m374,781\u001b[0m (1.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">374,781</span> (1.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m373,693\u001b[0m (1.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">373,693</span> (1.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,088\u001b[0m (4.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> (4.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DECODER ARCHITECTURE\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ decoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9984\u001b[0m)           │     \u001b[38;5;34m1,257,984\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_reshape (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm1 (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm_bn1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm2 (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm_bn2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm3 (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm_bn3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_upsample1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mUpSampling1D\u001b[0m)                  │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_conv1 (\u001b[38;5;33mConv1DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m82,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_bn1                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_relu1 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_upsample2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m624\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mUpSampling1D\u001b[0m)                  │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_conv2 (\u001b[38;5;33mConv1DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m624\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m41,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_bn2                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m624\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_relu2 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m624\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_upsample3               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mUpSampling1D\u001b[0m)                  │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_conv3 (\u001b[38;5;33mConv1DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m10,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_bn3                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_relu3 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_padding (\u001b[38;5;33mZeroPadding1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_output (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m3\u001b[0m)        │           \u001b[38;5;34m291\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ decoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9984</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,257,984</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm_bn1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm_bn2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_lstm_bn3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_upsample1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>)                  │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_bn1                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_upsample2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">624</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>)                  │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">624</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_bn2                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">624</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">624</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_upsample3               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>)                  │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_bn3                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_relu3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_padding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">291</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,657,219\u001b[0m (6.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,657,219</span> (6.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,656,131\u001b[0m (6.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,656,131</span> (6.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,088\u001b[0m (4.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> (4.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPLETE AUTOENCODER\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"vcg_autoencoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vcg_autoencoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ autoencoder_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m3\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m)            │       \u001b[38;5;34m374,781\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m3\u001b[0m)        │     \u001b[38;5;34m1,657,219\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ autoencoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">374,781</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,657,219</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,032,000\u001b[0m (7.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,032,000</span> (7.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,029,824\u001b[0m (7.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,029,824</span> (7.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,176\u001b[0m (8.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> (8.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model compiled with:\n",
            "  - Optimizer: Adam (lr=0.001)\n",
            "  - Loss: MSE\n",
            "  - Bottleneck size: 125\n",
            "  - Compression ratio: ~30:1\n",
            "\n",
            "[STEP 5] Training model...\n",
            "\n",
            "Starting training...\n",
            "  - Training samples: 7446\n",
            "  - Validation samples: 1315\n",
            "  - Epochs: 120\n",
            "  - Batch size: 32\n",
            "  - Input range: [-7.842, 8.414]\n",
            "Epoch 1/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.0089 - mae: 0.6838\n",
            "Epoch 1: val_loss improved from inf to 0.65425, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 75ms/step - loss: 1.0077 - mae: 0.6833 - val_loss: 0.6543 - val_mae: 0.5189 - learning_rate: 0.0010\n",
            "Epoch 2/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6001 - mae: 0.4657\n",
            "Epoch 2: val_loss improved from 0.65425 to 0.54334, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 66ms/step - loss: 0.6001 - mae: 0.4657 - val_loss: 0.5433 - val_mae: 0.4222 - learning_rate: 0.0010\n",
            "Epoch 3/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5836 - mae: 0.4547\n",
            "Epoch 3: val_loss improved from 0.54334 to 0.54327, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.5835 - mae: 0.4547 - val_loss: 0.5433 - val_mae: 0.4200 - learning_rate: 0.0010\n",
            "Epoch 4/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5553 - mae: 0.4366\n",
            "Epoch 4: val_loss improved from 0.54327 to 0.50850, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 66ms/step - loss: 0.5553 - mae: 0.4366 - val_loss: 0.5085 - val_mae: 0.3888 - learning_rate: 0.0010\n",
            "Epoch 5/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5428 - mae: 0.4275\n",
            "Epoch 5: val_loss did not improve from 0.50850\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 65ms/step - loss: 0.5428 - mae: 0.4275 - val_loss: 0.5211 - val_mae: 0.4116 - learning_rate: 0.0010\n",
            "Epoch 6/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5457 - mae: 0.4316\n",
            "Epoch 6: val_loss improved from 0.50850 to 0.48836, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.5457 - mae: 0.4316 - val_loss: 0.4884 - val_mae: 0.3726 - learning_rate: 0.0010\n",
            "Epoch 7/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5342 - mae: 0.4213\n",
            "Epoch 7: val_loss improved from 0.48836 to 0.47208, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.5342 - mae: 0.4213 - val_loss: 0.4721 - val_mae: 0.3683 - learning_rate: 0.0010\n",
            "Epoch 8/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5210 - mae: 0.4112\n",
            "Epoch 8: val_loss improved from 0.47208 to 0.46547, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.5210 - mae: 0.4112 - val_loss: 0.4655 - val_mae: 0.3567 - learning_rate: 0.0010\n",
            "Epoch 9/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.5132 - mae: 0.4105\n",
            "Epoch 9: val_loss did not improve from 0.46547\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.5132 - mae: 0.4105 - val_loss: 0.4847 - val_mae: 0.3989 - learning_rate: 0.0010\n",
            "Epoch 10/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5070 - mae: 0.4104\n",
            "Epoch 10: val_loss improved from 0.46547 to 0.45448, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.5070 - mae: 0.4104 - val_loss: 0.4545 - val_mae: 0.3570 - learning_rate: 0.0010\n",
            "Epoch 11/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5026 - mae: 0.4112\n",
            "Epoch 11: val_loss improved from 0.45448 to 0.44530, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.5026 - mae: 0.4112 - val_loss: 0.4453 - val_mae: 0.3572 - learning_rate: 0.0010\n",
            "Epoch 12/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5021 - mae: 0.4102\n",
            "Epoch 12: val_loss improved from 0.44530 to 0.43576, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.5021 - mae: 0.4102 - val_loss: 0.4358 - val_mae: 0.3557 - learning_rate: 0.0010\n",
            "Epoch 13/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5008 - mae: 0.4132\n",
            "Epoch 13: val_loss did not improve from 0.43576\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.5008 - mae: 0.4132 - val_loss: 0.4439 - val_mae: 0.3624 - learning_rate: 0.0010\n",
            "Epoch 14/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4831 - mae: 0.3987\n",
            "Epoch 14: val_loss did not improve from 0.43576\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.4831 - mae: 0.3987 - val_loss: 0.4479 - val_mae: 0.3749 - learning_rate: 0.0010\n",
            "Epoch 15/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.4810 - mae: 0.4028\n",
            "Epoch 15: val_loss improved from 0.43576 to 0.41903, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 73ms/step - loss: 0.4810 - mae: 0.4028 - val_loss: 0.4190 - val_mae: 0.3389 - learning_rate: 0.0010\n",
            "Epoch 16/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4631 - mae: 0.3922\n",
            "Epoch 16: val_loss improved from 0.41903 to 0.40313, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.4631 - mae: 0.3922 - val_loss: 0.4031 - val_mae: 0.3378 - learning_rate: 0.0010\n",
            "Epoch 17/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4542 - mae: 0.3871\n",
            "Epoch 17: val_loss improved from 0.40313 to 0.39153, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.4542 - mae: 0.3871 - val_loss: 0.3915 - val_mae: 0.3348 - learning_rate: 0.0010\n",
            "Epoch 18/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4532 - mae: 0.3892\n",
            "Epoch 18: val_loss improved from 0.39153 to 0.38671, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.4532 - mae: 0.3891 - val_loss: 0.3867 - val_mae: 0.3340 - learning_rate: 0.0010\n",
            "Epoch 19/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4381 - mae: 0.3828\n",
            "Epoch 19: val_loss improved from 0.38671 to 0.38282, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.4381 - mae: 0.3828 - val_loss: 0.3828 - val_mae: 0.3291 - learning_rate: 0.0010\n",
            "Epoch 20/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4292 - mae: 0.3815\n",
            "Epoch 20: val_loss improved from 0.38282 to 0.37000, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.4292 - mae: 0.3815 - val_loss: 0.3700 - val_mae: 0.3305 - learning_rate: 0.0010\n",
            "Epoch 21/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4164 - mae: 0.3793\n",
            "Epoch 21: val_loss improved from 0.37000 to 0.35504, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.4165 - mae: 0.3793 - val_loss: 0.3550 - val_mae: 0.3205 - learning_rate: 0.0010\n",
            "Epoch 22/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4109 - mae: 0.3769\n",
            "Epoch 22: val_loss did not improve from 0.35504\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.4109 - mae: 0.3769 - val_loss: 0.3650 - val_mae: 0.3244 - learning_rate: 0.0010\n",
            "Epoch 23/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4027 - mae: 0.3721\n",
            "Epoch 23: val_loss improved from 0.35504 to 0.33774, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 0.4027 - mae: 0.3721 - val_loss: 0.3377 - val_mae: 0.3242 - learning_rate: 0.0010\n",
            "Epoch 24/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3849 - mae: 0.3612\n",
            "Epoch 24: val_loss improved from 0.33774 to 0.32146, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.3849 - mae: 0.3612 - val_loss: 0.3215 - val_mae: 0.3030 - learning_rate: 0.0010\n",
            "Epoch 25/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3758 - mae: 0.3621\n",
            "Epoch 25: val_loss improved from 0.32146 to 0.30131, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - loss: 0.3758 - mae: 0.3621 - val_loss: 0.3013 - val_mae: 0.2956 - learning_rate: 0.0010\n",
            "Epoch 26/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3640 - mae: 0.3589\n",
            "Epoch 26: val_loss did not improve from 0.30131\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 69ms/step - loss: 0.3640 - mae: 0.3589 - val_loss: 0.3111 - val_mae: 0.3169 - learning_rate: 0.0010\n",
            "Epoch 27/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3558 - mae: 0.3513\n",
            "Epoch 27: val_loss improved from 0.30131 to 0.29660, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.3558 - mae: 0.3513 - val_loss: 0.2966 - val_mae: 0.3009 - learning_rate: 0.0010\n",
            "Epoch 28/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3518 - mae: 0.3518\n",
            "Epoch 28: val_loss improved from 0.29660 to 0.29304, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.3518 - mae: 0.3518 - val_loss: 0.2930 - val_mae: 0.2972 - learning_rate: 0.0010\n",
            "Epoch 29/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3307 - mae: 0.3396\n",
            "Epoch 29: val_loss improved from 0.29304 to 0.25346, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.3307 - mae: 0.3396 - val_loss: 0.2535 - val_mae: 0.2835 - learning_rate: 0.0010\n",
            "Epoch 30/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3242 - mae: 0.3386\n",
            "Epoch 30: val_loss improved from 0.25346 to 0.24487, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.3242 - mae: 0.3386 - val_loss: 0.2449 - val_mae: 0.2709 - learning_rate: 0.0010\n",
            "Epoch 31/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3128 - mae: 0.3310\n",
            "Epoch 31: val_loss improved from 0.24487 to 0.22853, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.3128 - mae: 0.3310 - val_loss: 0.2285 - val_mae: 0.2631 - learning_rate: 0.0010\n",
            "Epoch 32/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3035 - mae: 0.3311\n",
            "Epoch 32: val_loss did not improve from 0.22853\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.3035 - mae: 0.3311 - val_loss: 0.2368 - val_mae: 0.2594 - learning_rate: 0.0010\n",
            "Epoch 33/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2951 - mae: 0.3276\n",
            "Epoch 33: val_loss improved from 0.22853 to 0.22352, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.2951 - mae: 0.3276 - val_loss: 0.2235 - val_mae: 0.2746 - learning_rate: 0.0010\n",
            "Epoch 34/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2888 - mae: 0.3191\n",
            "Epoch 34: val_loss improved from 0.22352 to 0.22005, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.2888 - mae: 0.3191 - val_loss: 0.2201 - val_mae: 0.2657 - learning_rate: 0.0010\n",
            "Epoch 35/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2730 - mae: 0.3144\n",
            "Epoch 35: val_loss improved from 0.22005 to 0.20313, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.2730 - mae: 0.3144 - val_loss: 0.2031 - val_mae: 0.2615 - learning_rate: 0.0010\n",
            "Epoch 36/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2668 - mae: 0.3114\n",
            "Epoch 36: val_loss did not improve from 0.20313\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.2668 - mae: 0.3114 - val_loss: 0.2246 - val_mae: 0.2834 - learning_rate: 0.0010\n",
            "Epoch 37/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2557 - mae: 0.3039\n",
            "Epoch 37: val_loss improved from 0.20313 to 0.18013, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.2557 - mae: 0.3039 - val_loss: 0.1801 - val_mae: 0.2490 - learning_rate: 0.0010\n",
            "Epoch 38/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2519 - mae: 0.3017\n",
            "Epoch 38: val_loss did not improve from 0.18013\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.2519 - mae: 0.3017 - val_loss: 0.1914 - val_mae: 0.2374 - learning_rate: 0.0010\n",
            "Epoch 39/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2515 - mae: 0.3024\n",
            "Epoch 39: val_loss did not improve from 0.18013\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.2515 - mae: 0.3024 - val_loss: 0.1942 - val_mae: 0.2546 - learning_rate: 0.0010\n",
            "Epoch 40/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2429 - mae: 0.3000\n",
            "Epoch 40: val_loss improved from 0.18013 to 0.16888, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.2429 - mae: 0.3000 - val_loss: 0.1689 - val_mae: 0.2319 - learning_rate: 0.0010\n",
            "Epoch 41/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2296 - mae: 0.2933\n",
            "Epoch 41: val_loss did not improve from 0.16888\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 66ms/step - loss: 0.2296 - mae: 0.2933 - val_loss: 0.1961 - val_mae: 0.2481 - learning_rate: 0.0010\n",
            "Epoch 42/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2350 - mae: 0.2943\n",
            "Epoch 42: val_loss did not improve from 0.16888\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.2350 - mae: 0.2943 - val_loss: 0.1814 - val_mae: 0.2585 - learning_rate: 0.0010\n",
            "Epoch 43/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2271 - mae: 0.2899\n",
            "Epoch 43: val_loss improved from 0.16888 to 0.15130, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.2271 - mae: 0.2899 - val_loss: 0.1513 - val_mae: 0.2211 - learning_rate: 0.0010\n",
            "Epoch 44/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2126 - mae: 0.2807\n",
            "Epoch 44: val_loss did not improve from 0.15130\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 66ms/step - loss: 0.2126 - mae: 0.2807 - val_loss: 0.1643 - val_mae: 0.2325 - learning_rate: 0.0010\n",
            "Epoch 45/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2112 - mae: 0.2847\n",
            "Epoch 45: val_loss did not improve from 0.15130\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.2112 - mae: 0.2847 - val_loss: 0.1572 - val_mae: 0.2226 - learning_rate: 0.0010\n",
            "Epoch 46/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2081 - mae: 0.2816\n",
            "Epoch 46: val_loss did not improve from 0.15130\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.2081 - mae: 0.2816 - val_loss: 0.1559 - val_mae: 0.2381 - learning_rate: 0.0010\n",
            "Epoch 47/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1996 - mae: 0.2773\n",
            "Epoch 47: val_loss improved from 0.15130 to 0.15095, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1996 - mae: 0.2773 - val_loss: 0.1510 - val_mae: 0.2232 - learning_rate: 0.0010\n",
            "Epoch 48/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1929 - mae: 0.2719\n",
            "Epoch 48: val_loss did not improve from 0.15095\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1930 - mae: 0.2720 - val_loss: 0.1530 - val_mae: 0.2227 - learning_rate: 0.0010\n",
            "Epoch 49/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1937 - mae: 0.2707\n",
            "Epoch 49: val_loss improved from 0.15095 to 0.14786, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 67ms/step - loss: 0.1937 - mae: 0.2707 - val_loss: 0.1479 - val_mae: 0.2202 - learning_rate: 0.0010\n",
            "Epoch 50/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1839 - mae: 0.2660\n",
            "Epoch 50: val_loss improved from 0.14786 to 0.14502, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1839 - mae: 0.2660 - val_loss: 0.1450 - val_mae: 0.2188 - learning_rate: 0.0010\n",
            "Epoch 51/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1845 - mae: 0.2666\n",
            "Epoch 51: val_loss improved from 0.14502 to 0.13532, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1845 - mae: 0.2666 - val_loss: 0.1353 - val_mae: 0.2078 - learning_rate: 0.0010\n",
            "Epoch 52/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1775 - mae: 0.2647\n",
            "Epoch 52: val_loss did not improve from 0.13532\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1775 - mae: 0.2647 - val_loss: 0.1360 - val_mae: 0.2294 - learning_rate: 0.0010\n",
            "Epoch 53/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1758 - mae: 0.2616\n",
            "Epoch 53: val_loss did not improve from 0.13532\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.1758 - mae: 0.2616 - val_loss: 0.1407 - val_mae: 0.2228 - learning_rate: 0.0010\n",
            "Epoch 54/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1780 - mae: 0.2627\n",
            "Epoch 54: val_loss improved from 0.13532 to 0.11990, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1779 - mae: 0.2627 - val_loss: 0.1199 - val_mae: 0.2027 - learning_rate: 0.0010\n",
            "Epoch 55/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1711 - mae: 0.2612\n",
            "Epoch 55: val_loss did not improve from 0.11990\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 65ms/step - loss: 0.1711 - mae: 0.2612 - val_loss: 0.1384 - val_mae: 0.2155 - learning_rate: 0.0010\n",
            "Epoch 56/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1700 - mae: 0.2597\n",
            "Epoch 56: val_loss did not improve from 0.11990\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1700 - mae: 0.2597 - val_loss: 0.1252 - val_mae: 0.2087 - learning_rate: 0.0010\n",
            "Epoch 57/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1682 - mae: 0.2598\n",
            "Epoch 57: val_loss improved from 0.11990 to 0.11426, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1682 - mae: 0.2598 - val_loss: 0.1143 - val_mae: 0.2033 - learning_rate: 0.0010\n",
            "Epoch 58/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1610 - mae: 0.2563\n",
            "Epoch 58: val_loss did not improve from 0.11426\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1610 - mae: 0.2563 - val_loss: 0.1347 - val_mae: 0.2437 - learning_rate: 0.0010\n",
            "Epoch 59/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1645 - mae: 0.2589\n",
            "Epoch 59: val_loss did not improve from 0.11426\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 66ms/step - loss: 0.1645 - mae: 0.2589 - val_loss: 0.1173 - val_mae: 0.2094 - learning_rate: 0.0010\n",
            "Epoch 60/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1663 - mae: 0.2599\n",
            "Epoch 60: val_loss did not improve from 0.11426\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.1663 - mae: 0.2599 - val_loss: 0.1250 - val_mae: 0.2042 - learning_rate: 0.0010\n",
            "Epoch 61/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1594 - mae: 0.2541\n",
            "Epoch 61: val_loss did not improve from 0.11426\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.1594 - mae: 0.2541 - val_loss: 0.1221 - val_mae: 0.2057 - learning_rate: 0.0010\n",
            "Epoch 62/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1535 - mae: 0.2513\n",
            "Epoch 62: val_loss improved from 0.11426 to 0.11245, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.1535 - mae: 0.2513 - val_loss: 0.1124 - val_mae: 0.1964 - learning_rate: 0.0010\n",
            "Epoch 63/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1552 - mae: 0.2559\n",
            "Epoch 63: val_loss improved from 0.11245 to 0.11232, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1552 - mae: 0.2559 - val_loss: 0.1123 - val_mae: 0.2146 - learning_rate: 0.0010\n",
            "Epoch 64/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1496 - mae: 0.2508\n",
            "Epoch 64: val_loss did not improve from 0.11232\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.1496 - mae: 0.2507 - val_loss: 0.1156 - val_mae: 0.2204 - learning_rate: 0.0010\n",
            "Epoch 65/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1430 - mae: 0.2429\n",
            "Epoch 65: val_loss did not improve from 0.11232\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step - loss: 0.1431 - mae: 0.2430 - val_loss: 0.1140 - val_mae: 0.2106 - learning_rate: 0.0010\n",
            "Epoch 66/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1493 - mae: 0.2467\n",
            "Epoch 66: val_loss improved from 0.11232 to 0.09983, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1493 - mae: 0.2467 - val_loss: 0.0998 - val_mae: 0.1916 - learning_rate: 0.0010\n",
            "Epoch 67/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1469 - mae: 0.2502\n",
            "Epoch 67: val_loss did not improve from 0.09983\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.1469 - mae: 0.2502 - val_loss: 0.1014 - val_mae: 0.2006 - learning_rate: 0.0010\n",
            "Epoch 68/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1414 - mae: 0.2440\n",
            "Epoch 68: val_loss did not improve from 0.09983\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1414 - mae: 0.2440 - val_loss: 0.1049 - val_mae: 0.2038 - learning_rate: 0.0010\n",
            "Epoch 69/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1407 - mae: 0.2440\n",
            "Epoch 69: val_loss did not improve from 0.09983\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1407 - mae: 0.2440 - val_loss: 0.1090 - val_mae: 0.2090 - learning_rate: 0.0010\n",
            "Epoch 70/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1387 - mae: 0.2422\n",
            "Epoch 70: val_loss did not improve from 0.09983\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1387 - mae: 0.2422 - val_loss: 0.1001 - val_mae: 0.2042 - learning_rate: 0.0010\n",
            "Epoch 71/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1333 - mae: 0.2358\n",
            "Epoch 71: val_loss did not improve from 0.09983\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1334 - mae: 0.2358 - val_loss: 0.1111 - val_mae: 0.1911 - learning_rate: 0.0010\n",
            "Epoch 72/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1371 - mae: 0.2406\n",
            "Epoch 72: val_loss did not improve from 0.09983\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1371 - mae: 0.2406 - val_loss: 0.1203 - val_mae: 0.2083 - learning_rate: 0.0010\n",
            "Epoch 73/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1362 - mae: 0.2367\n",
            "Epoch 73: val_loss improved from 0.09983 to 0.09950, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.1362 - mae: 0.2367 - val_loss: 0.0995 - val_mae: 0.1976 - learning_rate: 0.0010\n",
            "Epoch 74/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1369 - mae: 0.2412\n",
            "Epoch 74: val_loss did not improve from 0.09950\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1369 - mae: 0.2412 - val_loss: 0.1061 - val_mae: 0.2001 - learning_rate: 0.0010\n",
            "Epoch 75/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1320 - mae: 0.2374\n",
            "Epoch 75: val_loss did not improve from 0.09950\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.1320 - mae: 0.2374 - val_loss: 0.1062 - val_mae: 0.1944 - learning_rate: 0.0010\n",
            "Epoch 76/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1272 - mae: 0.2340\n",
            "Epoch 76: val_loss improved from 0.09950 to 0.09537, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 74ms/step - loss: 0.1272 - mae: 0.2340 - val_loss: 0.0954 - val_mae: 0.1860 - learning_rate: 0.0010\n",
            "Epoch 77/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1287 - mae: 0.2322\n",
            "Epoch 77: val_loss did not improve from 0.09537\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.1287 - mae: 0.2322 - val_loss: 0.1167 - val_mae: 0.2106 - learning_rate: 0.0010\n",
            "Epoch 78/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1317 - mae: 0.2352\n",
            "Epoch 78: val_loss did not improve from 0.09537\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.1317 - mae: 0.2352 - val_loss: 0.1002 - val_mae: 0.1905 - learning_rate: 0.0010\n",
            "Epoch 79/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1242 - mae: 0.2304\n",
            "Epoch 79: val_loss improved from 0.09537 to 0.09247, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.1242 - mae: 0.2304 - val_loss: 0.0925 - val_mae: 0.1918 - learning_rate: 0.0010\n",
            "Epoch 80/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1286 - mae: 0.2383\n",
            "Epoch 80: val_loss did not improve from 0.09247\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.1286 - mae: 0.2383 - val_loss: 0.0925 - val_mae: 0.1897 - learning_rate: 0.0010\n",
            "Epoch 81/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1269 - mae: 0.2341\n",
            "Epoch 81: val_loss improved from 0.09247 to 0.08566, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 0.1269 - mae: 0.2341 - val_loss: 0.0857 - val_mae: 0.1767 - learning_rate: 0.0010\n",
            "Epoch 82/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1227 - mae: 0.2283\n",
            "Epoch 82: val_loss did not improve from 0.08566\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1227 - mae: 0.2283 - val_loss: 0.1378 - val_mae: 0.2123 - learning_rate: 0.0010\n",
            "Epoch 83/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1266 - mae: 0.2319\n",
            "Epoch 83: val_loss did not improve from 0.08566\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - loss: 0.1266 - mae: 0.2319 - val_loss: 0.0871 - val_mae: 0.1786 - learning_rate: 0.0010\n",
            "Epoch 84/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1211 - mae: 0.2294\n",
            "Epoch 84: val_loss did not improve from 0.08566\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.1211 - mae: 0.2294 - val_loss: 0.0930 - val_mae: 0.1953 - learning_rate: 0.0010\n",
            "Epoch 85/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1196 - mae: 0.2246\n",
            "Epoch 85: val_loss did not improve from 0.08566\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1197 - mae: 0.2246 - val_loss: 0.0967 - val_mae: 0.1946 - learning_rate: 0.0010\n",
            "Epoch 86/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1182 - mae: 0.2242\n",
            "Epoch 86: val_loss did not improve from 0.08566\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 67ms/step - loss: 0.1182 - mae: 0.2242 - val_loss: 0.0930 - val_mae: 0.1893 - learning_rate: 0.0010\n",
            "Epoch 87/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1164 - mae: 0.2260\n",
            "Epoch 87: val_loss improved from 0.08566 to 0.08335, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.1164 - mae: 0.2260 - val_loss: 0.0833 - val_mae: 0.1813 - learning_rate: 0.0010\n",
            "Epoch 88/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1165 - mae: 0.2255\n",
            "Epoch 88: val_loss did not improve from 0.08335\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1165 - mae: 0.2255 - val_loss: 0.1098 - val_mae: 0.2233 - learning_rate: 0.0010\n",
            "Epoch 89/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1139 - mae: 0.2219\n",
            "Epoch 89: val_loss did not improve from 0.08335\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1139 - mae: 0.2219 - val_loss: 0.0858 - val_mae: 0.1788 - learning_rate: 0.0010\n",
            "Epoch 90/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1162 - mae: 0.2231\n",
            "Epoch 90: val_loss did not improve from 0.08335\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1162 - mae: 0.2231 - val_loss: 0.1006 - val_mae: 0.1891 - learning_rate: 0.0010\n",
            "Epoch 91/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1139 - mae: 0.2210\n",
            "Epoch 91: val_loss did not improve from 0.08335\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1139 - mae: 0.2210 - val_loss: 0.0844 - val_mae: 0.1799 - learning_rate: 0.0010\n",
            "Epoch 92/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1141 - mae: 0.2213\n",
            "Epoch 92: val_loss did not improve from 0.08335\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 67ms/step - loss: 0.1141 - mae: 0.2213 - val_loss: 0.0951 - val_mae: 0.1871 - learning_rate: 0.0010\n",
            "Epoch 93/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1077 - mae: 0.2168\n",
            "Epoch 93: val_loss improved from 0.08335 to 0.08184, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 72ms/step - loss: 0.1077 - mae: 0.2168 - val_loss: 0.0818 - val_mae: 0.1792 - learning_rate: 0.0010\n",
            "Epoch 94/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1093 - mae: 0.2185\n",
            "Epoch 94: val_loss did not improve from 0.08184\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 68ms/step - loss: 0.1093 - mae: 0.2185 - val_loss: 0.0917 - val_mae: 0.1796 - learning_rate: 0.0010\n",
            "Epoch 95/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1117 - mae: 0.2198\n",
            "Epoch 95: val_loss did not improve from 0.08184\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1117 - mae: 0.2198 - val_loss: 0.0901 - val_mae: 0.1862 - learning_rate: 0.0010\n",
            "Epoch 96/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1073 - mae: 0.2152\n",
            "Epoch 96: val_loss did not improve from 0.08184\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 67ms/step - loss: 0.1073 - mae: 0.2152 - val_loss: 0.0835 - val_mae: 0.1761 - learning_rate: 0.0010\n",
            "Epoch 97/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1083 - mae: 0.2192\n",
            "Epoch 97: val_loss improved from 0.08184 to 0.07373, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 68ms/step - loss: 0.1083 - mae: 0.2192 - val_loss: 0.0737 - val_mae: 0.1751 - learning_rate: 0.0010\n",
            "Epoch 98/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1025 - mae: 0.2135\n",
            "Epoch 98: val_loss did not improve from 0.07373\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - loss: 0.1025 - mae: 0.2135 - val_loss: 0.0914 - val_mae: 0.1822 - learning_rate: 0.0010\n",
            "Epoch 99/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1094 - mae: 0.2172\n",
            "Epoch 99: val_loss did not improve from 0.07373\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.1094 - mae: 0.2172 - val_loss: 0.0776 - val_mae: 0.1801 - learning_rate: 0.0010\n",
            "Epoch 100/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1025 - mae: 0.2121\n",
            "Epoch 100: val_loss did not improve from 0.07373\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - loss: 0.1025 - mae: 0.2121 - val_loss: 0.1026 - val_mae: 0.2176 - learning_rate: 0.0010\n",
            "Epoch 101/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1045 - mae: 0.2153\n",
            "Epoch 101: val_loss did not improve from 0.07373\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 68ms/step - loss: 0.1045 - mae: 0.2153 - val_loss: 0.0756 - val_mae: 0.1661 - learning_rate: 0.0010\n",
            "Epoch 102/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1003 - mae: 0.2097\n",
            "Epoch 102: val_loss did not improve from 0.07373\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.1003 - mae: 0.2097 - val_loss: 0.0806 - val_mae: 0.1703 - learning_rate: 0.0010\n",
            "Epoch 103/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1081 - mae: 0.2143\n",
            "Epoch 103: val_loss improved from 0.07373 to 0.06974, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.1081 - mae: 0.2143 - val_loss: 0.0697 - val_mae: 0.1645 - learning_rate: 0.0010\n",
            "Epoch 104/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1002 - mae: 0.2113\n",
            "Epoch 104: val_loss did not improve from 0.06974\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - loss: 0.1002 - mae: 0.2113 - val_loss: 0.0773 - val_mae: 0.1755 - learning_rate: 0.0010\n",
            "Epoch 105/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1005 - mae: 0.2111\n",
            "Epoch 105: val_loss did not improve from 0.06974\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1005 - mae: 0.2111 - val_loss: 0.0824 - val_mae: 0.1845 - learning_rate: 0.0010\n",
            "Epoch 106/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0988 - mae: 0.2095\n",
            "Epoch 106: val_loss improved from 0.06974 to 0.06841, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 73ms/step - loss: 0.0988 - mae: 0.2095 - val_loss: 0.0684 - val_mae: 0.1624 - learning_rate: 0.0010\n",
            "Epoch 107/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0968 - mae: 0.2095\n",
            "Epoch 107: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.0968 - mae: 0.2095 - val_loss: 0.0891 - val_mae: 0.1814 - learning_rate: 0.0010\n",
            "Epoch 108/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0955 - mae: 0.2040\n",
            "Epoch 108: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.0955 - mae: 0.2040 - val_loss: 0.0858 - val_mae: 0.1934 - learning_rate: 0.0010\n",
            "Epoch 109/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0983 - mae: 0.2089\n",
            "Epoch 109: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.0983 - mae: 0.2089 - val_loss: 0.0876 - val_mae: 0.2010 - learning_rate: 0.0010\n",
            "Epoch 110/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0980 - mae: 0.2074\n",
            "Epoch 110: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 73ms/step - loss: 0.0980 - mae: 0.2074 - val_loss: 0.0717 - val_mae: 0.1677 - learning_rate: 0.0010\n",
            "Epoch 111/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0960 - mae: 0.2057\n",
            "Epoch 111: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.0960 - mae: 0.2058 - val_loss: 0.0741 - val_mae: 0.1664 - learning_rate: 0.0010\n",
            "Epoch 112/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0934 - mae: 0.2021\n",
            "Epoch 112: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 0.0934 - mae: 0.2021 - val_loss: 0.1120 - val_mae: 0.2292 - learning_rate: 0.0010\n",
            "Epoch 113/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0994 - mae: 0.2103\n",
            "Epoch 113: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 0.0994 - mae: 0.2103 - val_loss: 0.0772 - val_mae: 0.1811 - learning_rate: 0.0010\n",
            "Epoch 114/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0915 - mae: 0.2016\n",
            "Epoch 114: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.0915 - mae: 0.2016 - val_loss: 0.0741 - val_mae: 0.1721 - learning_rate: 0.0010\n",
            "Epoch 115/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0949 - mae: 0.2049\n",
            "Epoch 115: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 73ms/step - loss: 0.0949 - mae: 0.2049 - val_loss: 0.0760 - val_mae: 0.1729 - learning_rate: 0.0010\n",
            "Epoch 116/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0940 - mae: 0.2059\n",
            "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 116: val_loss did not improve from 0.06841\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.0940 - mae: 0.2059 - val_loss: 0.0719 - val_mae: 0.1740 - learning_rate: 0.0010\n",
            "Epoch 117/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0821 - mae: 0.1930\n",
            "Epoch 117: val_loss improved from 0.06841 to 0.05460, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - loss: 0.0821 - mae: 0.1930 - val_loss: 0.0546 - val_mae: 0.1441 - learning_rate: 5.0000e-04\n",
            "Epoch 118/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0801 - mae: 0.1919\n",
            "Epoch 118: val_loss did not improve from 0.05460\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 73ms/step - loss: 0.0801 - mae: 0.1919 - val_loss: 0.0553 - val_mae: 0.1506 - learning_rate: 5.0000e-04\n",
            "Epoch 119/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0746 - mae: 0.1854\n",
            "Epoch 119: val_loss improved from 0.05460 to 0.05242, saving model to best_vcg_autoencoder.keras\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.0746 - mae: 0.1854 - val_loss: 0.0524 - val_mae: 0.1391 - learning_rate: 5.0000e-04\n",
            "Epoch 120/120\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0768 - mae: 0.1881\n",
            "Epoch 120: val_loss did not improve from 0.05242\n",
            "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 0.0768 - mae: 0.1882 - val_loss: 0.0586 - val_mae: 0.1506 - learning_rate: 5.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 119.\n",
            "\n",
            "[STEP 6] Evaluating performance...\n",
            "\n",
            "[AVERAGE PERFORMANCE ACROSS TEST SAMPLES]\n",
            "\n",
            "======================================================================\n",
            "PERFORMANCE METRICS\n",
            "======================================================================\n",
            "Compression Ratio (CR):           30.00\n",
            "Mean Squared Error (MSE):         0.060258\n",
            "Root Mean Square Error (RMSE):    0.2353\n",
            "Normalized MSE (NMSE):            0.182263\n",
            "Percentage RMS Difference (PRD):  26.97%\n",
            "PRD Normalized (PRDN):            2.08\n",
            "Signal-to-Noise Ratio (SNR):      12.01 dB\n",
            "Peak SNR (PSNR):                  24.31 dB\n",
            "Quality Score (QS):               1.29\n",
            "======================================================================\n",
            "\n",
            "[STEP 7] Creating visualizations...\n",
            "  ✓ Saved: training_history.png\n",
            "  ✓ Saved: vcg_comparison.png\n",
            "  ✓ Saved: vcg_3d_original.png\n",
            "  ✓ Saved: vcg_3d_reconstructed.png\n",
            "\n",
            "[STEP 8] Saving models...\n",
            "Models saved: vcg_encoder.keras, vcg_decoder.keras\n",
            "\n",
            "======================================================================\n",
            "PIPELINE COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Generated files:\n",
            "  - best_vcg_autoencoder.keras (Complete model)\n",
            "  - vcg_encoder.keras (Encoder only)\n",
            "  - vcg_decoder.keras (Decoder only)\n",
            "  - vcg_autoencoder_pi.tflite (Raspberry Pi optimized)\n",
            "\n",
            "Visualization files:\n",
            "  - training_history.png\n",
            "  - vcg_comparison.png\n",
            "  - vcg_3d_original.png\n",
            "  - vcg_3d_reconstructed.png\n",
            "\n",
            "To view visualizations, open the PNG files in your file explorer.\n",
            "\n",
            "📊 Performance Summary:\n",
            "    Compression Ratio: 30.00:1\n",
            "    PRD: 26.97%\n",
            "    SNR: 12.01 dB\n",
            "    Quality Score: 1.29\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}